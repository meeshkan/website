---
title: Energy-based computing
description: A new (old) way to think about computing
slug: energy-based-computing
date: 2020-08-07
authors: ["mike"]
published: true
tags:
  - lambda calculus
  - category theory
  - turing complete
---

I just got back from a two-week vacation during which I had time to binge [watch](https://www.youtube.com/watch?v=I8LbkfSSR58&list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_)/[read](https://www.blurb.com/b/9621951-category-theory-for-programmers-new-edition-hardco) Bartosz Milewski's [Category Theory for Programmers](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/).  My work using [profunctor lenses](https://github.com/purescript-contrib/purescript-profunctor-lenses) to create GraphQL [validators](https://github.com/meeshkan/purescript-graphql-validator) and [generators](https://github.com/meeshkan/purescript-graphql-gen) led me to asking "What is a profunctor, anyway, and why are they used to make lenses?", which got me to Bartosz's [article on profunctor optics](https://bartoszmilewski.com/2017/07/07/profunctor-optics-the-categorical-view/).  As this was way over my head, I started with his [first blog post on category theory](https://bartoszmilewski.com/2014/11/04/category-the-essence-of-composition/), working my way little by little until I got back to this article.  Miraculously, when I got back, I understood it, which is a testament to Bartosz's incredible talent as a pedagogue and his deep understanding of category theory.  To anyone that programs regularly, I would _strongly recommend_ watching Bartosz's lectures and reading his blog.  You'll be a better programmer for it, and it will change the way you think about problem solving.

One question that came to mind while watching Bartosz's lectures was using Category Theory as a basis for reasoning about energy-based computing.  Energy-based computing is the technique I've been using to implement the [Wall programming language](https://wall-lang.netlify.app/), my eternally-unfinished side project.  Wall is an alternative to dependently-typed languages like [Agda](https://wiki.portal.chalmers.se/agda/pmwiki.php) and [Idris](https://www.idris-lang.org/) that keeps track of functions' potential domains and co-domains using the [Z3 theorem prover](https://github.com/Z3Prover/z3), raising a compilation error if a function is evaluated with an argument that may be outside of its domain.  Doing so imposes heavy restrictions on what functions can and can't be executed in Wall.  The way I manage these restrictions is by reasoning about function execution/evaluation in terms of _energy_.

## Energy-based models

The idea behind energy-based computation is embarassingly simple.  A computing system (ie a program that adds two numbers) is "filled up" with a limited amount of energy, and each unit of computation consumes a unit of energy.  Note that this energy is purely conceptual: I don't mean that you literally spin a crank to charge up a computer program and let it run (although that'd be cool!), but rather that the program has a limited supply of virtual energy that it consumes as it executes.  The computation terminates _either_ when a program is killed _or_ when it runs out of energy.

In truth, all programs are _de facto_ energy-based.  At some point, either I will close the VSCode editor in which I am writing this blog post _or_ VSCode will shut down for lack of energy, either because there is a power outage or because I unplug the computer or because the Sun swallows the Earth or some other cataclysmic event that would result in VSCode's closing.

An energy-based model of computation makes the potential outcomes of a program completely decideable, if not unfathomably large.  It just requires the right level of granularity.  For example, if my program adds two natural numbers, I need enough energy to:

1. Construct both numbers.
2. Add them.

If the program doesn't have enough energy, it will putter out, returning some sort of [bottom](https://en.wikipedia.org/wiki/Bottom_type).  The symbol `‚ä•` is often used for bottom in computing, and I like to think of it as "the program giving you the middle finger", meaning it's saying "I'm out of energy, I give up, screw you."

In this way, programs are restricted to finite values, but the finite values shrink and expand depending on the path the program takes.  For example, if all the program does is add two numbers, I can add two _really big_ natural numbers because all the energy can be spent on their construction and their subsequent addition.  Depending on the system being used, the construction of natural numbers can be energy-intensive (ie the [Peano representation](https://en.wikipedia.org/wiki/Peano_axioms)) or cheap (ie using vectors of bits).  But the general idea is that programs are born with a fixed amount of energy _by design_.  This is already the case when running dapps on [Ethereum](https://ethereum.org/en/), and the fact that the internal pricing mechanism of Ethereum is called [gas](https://www.investopedia.com/terms/g/gas-ethereum.asp) shows that ideas of energy-based computation are already at the heart of certain computing processes.

What energy-based models afford you is the ability to _prove_ that a program can or cannot get into a certain state.  This is because the potential states of a program are completely bound by the amount of energy that goes into them.  Of course, proofs are possible in systems with limitless energy - you can proove, for example, that [reversing a list twice yields the original list](https://softwarefoundations.cis.upenn.edu/current/lf-current/Lists.html) irrespective of the length of the list (ie even if it is infinitely long, which requires limitless energy).  But these proofs are often tedious, sometimes impossible, and not (yet) fully automatable.  In my limited experience building Wall, I've found that using models of bounded energy make proofs easier to automate and reuse.  The tricky part is proving isomorphisms between results at different energy levels (ie proving that 5+4 when you have 100 units of energy at your disposal is the same as 5+4 when you have 99 units of energy at your disposal), but this is doable with the right amount of elbow-grease and pixy-dust, and for anyone interested, you can check out the [code of the initial implementation on GitHub](https://github.com/mikesol/wall/tree/wall).

## Energizing category theory

Back to Bartosz's lectures, one point he makes early on is that, when programmers think in categorical terms, they often ignore time.  Proofs rarely have an asterisk next to them saying "while this holds, it will take a billion years to compute."  This makes category theory quite powerful in an abstract sense, but leads to some imperfect compromises in the real-world, ie the use of a category called **Hask** in Haskell that grafts an "infinite-loop" type onto every other type to represent a non-terminating computation.  Of course, this is not an inherent limitation of categorical theory: it just means that the prevailing categories that are used and the prevailing conceptual concetps ([universal properties](https://en.wikipedia.org/wiki/Universal_property), [ends](https://en.wikipedia.org/wiki/End_(category_theory)), [adjunctions](https://en.wikipedia.org/wiki/Adjoint_functors) etc) are, for better or for worse, born with infinite energy/time.

In fact, adding energy/time to category theory seems like a pretty unpalletable idea, at least on the surface.  Take, for example, one of the workhorses of categorical thinking: the [monoid](https://en.wikipedia.org/wiki/Monoid_(category_theory)).  In really simple terms, a monoid is a set (ie integers) coupled with an associative operation (ie addition) and a unit under association (ie 0 in addition).  There is no limit to the amount of times you can apply this associative operation - you'll always get a result that is a member of your set (ie 3+4=7, which is itself an integer, which can be added to another number).  Adding energy to a monoid effectively un-monoids it, at least in the way we understand monoids.  There are no longer limitless associative operations.  So if energy-based categories destroy things like monoids, what can they afford us?

I have two fledgling, ill-formed intuitive responses to this question.  One is that energy-based computation systems are just [directed acyclic graphs](https://en.wikipedia.org/wiki/Directed_acyclic_graph), which means that they are partial orders, and there is a [substrate of category theory dealing with partial orders](https://en.wikipedia.org/wiki/Order_theory#Category_theory) that may be applicable here (I'm currently slowly making my way through [this](https://link.springer.com/chapter/10.1007/BFb0026983) and [this](https://link.springer.com/chapter/10.1007/978-3-642-38164-5_4), which max out my brain but are helpful in thinking through this idea).  Another intuition is that energy-based computation systems can be thought of as structures containing a bunch of different potential operations, meaning that each one forms an [F-algebra](https://en.wikipedia.org/wiki/F-algebra), albeit a degenerate one without a fixed point.  Perhaps reasoning about these F-algebras using standard tools (ie adjunctions) could be a way forward.

Of course, category theory is not the only way to reason about programs.  Another way to model energy in programs is to take the lambda calculus and add an axiom of energy so that terms that traditionally do not terminate through [beta-reduction](https://en.wikipedia.org/wiki/Lambda_calculus#.CE.B2-reduction), ie Omega, are assigned finite energy levels.  Omega 100, for example, can apply itself to itself 100 times before calling it quits.  However, as category theory will be my "flavor of the week" for _n_ weeks until my brain hurts too much or or I get bored/distracted, I'm enjoying thinking through energy in categorical terms.

## Why does this matter?

At Meeshkan, our tl;dr is that we create models of software and then verify that a peice of software corresponds to its model.  The process is kinda like surveying a building, then drawing what you think would have been its blueprint, and then verifying that the building conforms to your reverse-engineered blueprint.  The hardest part of this is building the blueprint from the building, and that's where I think energy-based computation can really excel.  Energy-based computation will produce a blueprint that is guaranteed to have a maximal size and/or degree of granularity, which helps you compare different parts of it, reutilize chunks of it, etc.

Over the next year(s), I'm looking forward to developing this idea further and seeing if it can help answer some of the toughest challenges we face at Meeshkan, such as:

1. What changes to an energy-bound program preserve an isomorphic relationship to the original energy-bound program, perhaps with more or less energy added?  If we can answer this, then we can make a distinction between optimizations/refactorings and breaking-changes/features/regressions.  This is usually really hard and requires recourse to [Hoare Logic](https://en.wikipedia.org/wiki/Hoare_logic), but my hunch is that an energy-based system will simplify things a great deal for programs for which you can specify an upper limit of acceptable execution time?
1. An energy-based model can help identify a series of performance-based regressions without exhaustive benchmarking.  Traditionally, model-based tests can determine if a program is behaving badly, but it is not good at determining if a program will be too slow in certain circumstances.  In many industries, sluggishness is as bad as, or worse than, buggy results, and I think an energy-based model can help us better reason about that in our tests.
1. Coming up with better test cases.  Model-based testing suffers from the "garbage-in, garbage-out" problem where crappy test cases can't really say anything interesting about the system under test.  An energy-based approach to modeling systems can better identify logical groups of test cases by looking at energy flows (ie using [spectral-graph theory](https://en.wikipedia.org/wiki/Spectral_graph_theory)), which ensures that if we run 10,000 tests, they'll cover an adequate surface area of the system under test.
1. Shrinking.  No one likes a bug report with incomprehensible data and endless stack traces.  An energy-based model of computation can allow one to reason about parts of programs that isomorphic to each-other, allowing one to shrink-down a code path that provokes a bug to something more manageable.

In brief, I'm excited enough about this idea and its potential implications at Meeshkan to keep toying around with it when I'm not busy running the company (meaning between 2-3AM on Thursdays every other week provided that I'm not on call).  Of course, if anyone else is interested in keeping the conversation going, I'd love to know!