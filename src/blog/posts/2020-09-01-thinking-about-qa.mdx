---
title: When to start thinking about QA
description: Good times in a company's lifecycle to start - and stop - thinking about QA.
slug: thinking-about-qa
date: 2020-08-20
authors: ["mike"]
published: true
tags:
  - qa
  - api
---

At a conference recently, I heard someone ask the question "When's a good time to start thinking about QA?" The question was asked in earnest, but the speaker responded with a quick "Immediately!" and moved onto the next question. While "Immediately!" reflected the speaker's enthusiasm about quality assurance, it left the question-asker down in the dumps.

Knowing when to think about QA is _really hard_, and there's not that much good advice out there. A lot of blog posts, like the above-mentioned speaker, skirt the issue by saying things like "write more tests" or "use strong typing." Chances are, if you're Googling "When to start thinking about QA", these are not answers to your question. This article _is_.

## A couple definitions

Once, when I was building an Android app, an angry supervisor asked us to "Stop throwing the app over the net to QA." I understood what the person meant, and I've heard this sort of critique a lot. The issue with the critique is that the most important part of the QA process _is_ throwing your app over the net. The reason you throw your app over the net is because, very soon, you will be throwing your app over an even bigger net called production.

I like to use the analogy of the performing arts here: there are lots of rehearsals where you bring in someone from the outside to look at your work. The last rehearsal (the dress rehearsal) is almost always this, but there are regular points in the development of a staged production where you'll bring in someone from outside the team to give their opinion. You do this because, eventually, there will be hundreds and thousands of people forming their own opinion about your work, and it's important to make sure that you get across the elements they need to form an opinion. When you work with a QA engineer, you're bringing someone in to watch a rehearsal.

A lot of people worry that QA engineers will treat every rehearsal like the dress rehearsal, but this is not true. Great QA engineers know exactly what stage a project or feature is at and will give feedback that will help the project or feature _at that stage_.

So to sum up, QA is a third-party viewpoint that gives you stage-appropriate technical feedback about your project.

## The single most important step

The most important step you can take, starting today, is kicking off a relationship with a QA professional or a company providing QA. This does not require purchasing or using a service - it just means introducing yourself.

Great QA services (and we like to think we're building one!) know how to take it from here. Like a tailor or a doctor, they will start taking measures. By monitoring a codebase, a roadmap, and specs, a QA professional will focus on the following questions:

- When are major releases planned?
- Are there any indicators that would predict increased development velocity at a given time?
- How often are there breaking changes to a service, and what are the precursors to these changes?

These questions are asked in order to plan out when their help can be most impactful and necessary.

At Meeshkan, we ask and answer these questions by testing code bases, creating specs, and building burn charts. The purpose of this is not to provide you with information (you're already overloaded!) but to answer even more important questions:

- When will the team likely have time to digest test reports?
- Are there enough valuable indicators about quality to share them with the team?
- What is a five-alarm fire in the context of the team's business needs?

All of this means that, when you get a ping from us, you can trust us that it is important. Of course, we give you access to the entire trail that leads up to our decisions, and if you really want to, you can read over the tens of thousands of tests we run on your service. But our job is knowing _what_ to present to you, _when_ to present it, and _how_ it should be presented.

So, [introduce yourself](./). Say hi. It will take a matter of minutes, and you'll take the first step in building a the long-term relationship that underpins great QA.

## The next step

Introducing yourself is already half the battle: it will aleviate a lot of the burden of thinking about QA because your trusted partner knows when to get involved and when to get out of the way.

The next step, and the other half of the battle, is what happens at the "rehearsal", to use the analogy from above. What are you hoping to get out of it that you wouldn't get out of writing a few unit tests or using stronger typing?

QA is all about business outcomes: it puts itself in the place of the user, looks at the current state of the UX, and assesses if the "diff" between the current UX and the desired UX is on track. If a QA engineer crashes your app by pressing a button five times in rapid succession, they won't elevate the crash unless it is linked to an underlying problem that can affect business outcomes.

We see this all the time at Meeshkan - our algorithms regularly crash servers with very large numbers, negative pagination and other forms of exotic input. If you hear about it from us on a top-level report, it is because we've linked the crashes to some underlying behavior that we think is unsustainable. Otherwise, you won't hear from us because _it's ok if your app crashes in the wild_. Don't let anyone tell you otherwise! What's not OK is for your business to consistently only ever return three out of twenty valid search results to a user. That's not a crash, but if search is a critical feature of your business, then that's a major problem. Great QA brings that to the fore.

The difference between Meeshkan and traditional manual QA is that we do this automatically. There are several core benefits of automated QA:

- It explores a _lot_ of cases. This is _quantity_ assurance, which underpins quality assurance because you build a better understanding the more you work with the system under test.
- It pools data from hundreds of cases to recognize patterns and make the best possible predictions.
- It moves at the speed of your development by testing every change to source code.
- The price point makes it easy for small businesses to get started.

What automated QA is _not_ a replacement for is manual QA. Someone will always need to give your app or service a spin in the wild. But automated QA leads naturally to manual QA in several respects:

- It gives a tester a machine-built spec, which can act as a map and save them time.
- It shows what parts of the app are the most trivial/predictable so that the manual tester can focus on the hard stuff.
- Like drivers in a self-driving car, manual testers provide a valuable source of confirmation to machine-built QA algorithms, creating a feedback loop for unbeatable value.

## An example

I've seen enough small businesses get started with QA to know exactly what types of problems crop up and what an initial collaboration looks like. As you read this, ask yourself "Does this sound like my business"? If so, get in touch!

A business deploys a webapp that has a few features. Initially, all of the features are in your head, and you're able to manually test the app and write unit tests that give you confidence when you deploy.

Soon, the app grows in complexity as do the underlying services. At a certain point, you start to feel an uneasy feeling when you merge a branch to production. This feeling feels odd: you've tested the service as best you could, all the tests are passing, and still you imagine breakage occurring, although you're not quite sure what. It could be a logged out user accessing logged-in resources. It could be an API sending back. It could be JSON parsing failure leading to serving a `404`. But there is some form of psychological resistance to merging to production.

The day you feel this is the day to kick off a conversation with a potential QA partner. If we're fortunate enough to earn your business, one of the first things we'll do is identify all of the points in your app that are most likely making you feel this way. For example, if you use some form of message passing like an Erlang cluster or Google PubSub and we see you use it in a certain way to tie together your app, we will lean in and test that more because we've identified it is a common source of fragility (and, inversely, a great strength when bug-free!).

These tests don't just bombard the system with garbage input. For example, we will look to see if there are PubSub topics that are written to but never read or topics that are read but never written to. We will also try to find common mispellings of PubSub channels and potential deadlocks of two channels mutually waiting for each other. You may get a report that says the following:

```
Channel `customerPaid` expects input written to channel `customerRegistered`, but `customerRegistered` expects input to `customerPaid`.
```

We may send you _only_ this in the report _even though_ your app is crashing in other places. This gets back to the rehearsal idea from above - our goal is not to taunt you with the 1,000 ways we made your service crash, but to give you a few pearls of information that will actually make your service _better_.

Of course, the danger of giving an example is that it excludes others. PubSub is one of many things we're looking at - CRUD operations, calls to third-party APIs, field names, and unused code paths are all things that may trigger a report.

When you're working with manual QA, you may resist bringing them in because you're still testing out your PubSub (or whatever) implementation and are not sure if it's worth testing yet. _With Meeshkan, we help make your idea, however fragile, worth testing!_

## How much does this all cost?

I've sat through meetings where engineers are justifying spending money on testing to their boss, and it's interesting to see the giant chasm that exists between technical and non-technical folks here. People that don't build technology _totally_ get that everything needs to be tested, and are usually shocked to learn that there are giant patches of a service that are more or less untested. The general vibe is: "We're paying you to write code that works, so if that means testing it, than test it, but we shouldn't have to pay extra for testing."

The thing is, they're right. They're paying you to write code that works. So conceiving of the cost of QA as a supplement to development is not a good idea. I encourage folks to think of QA as market research with a focus group that mimics _every_ segment you care about. It's like hiring great actors to play the role of your users and, when they get off the stage, report back to you how you could have written the play better.

With this in mind, my rule of thumb is to plan one full-time salary going into some form of QA for every ~6 engineering hires. The math goes like this. Three people can generally develop a robust MVP or feaature in a month, and you want them to be nose down in doing it. As this core team progresses, you want one person observing the development from the vantage point of the entity that will ultimately interact with the service, reporting back so that building stays on track. It will save approximately 1 week of work for the team after launching the feature.

So, one week of work saved for two teams of three equals six weeks of work saved for four weeks of work put in by your QA. Your save exceeds your spend by ~1.5

As mentioned above, the most important thing is developing a relationship with this third-party service early on so that you can scale up and down naturally. At Meeshkan, one of the benefits of automated QA is that the less you have going on, the less you pay, and if there's nothing worth testing then we don't charge.

So, if you're making your budget, plan on 1 full-time salary spent on QA for every ~6 engineers, and if you can use an automated service that scales like SaaS (like Meeshkan!), it is a good way to start small even from 1-2 engineers.

## How to get started

Book a free consultation or subscribe to our newsletter below. You won't regret it!

## Conclusion

The purpose of this article, and why it is a bit longer than your average missive on the subject, is to avoid a kitch or cop-out answer to "When should I start thinking about QA?" It explores this question head on, addressing the purpose of QA, the benefits you can expect, the costs to anticipate and how to kick things off.

As businesses increasingly find themselves pivoting to keep up with market tendencies, the major challenge of the next decade will not be making software that's built to last. It will be honing in on market signals faster than competitors. Your software is the _best_ signal detector there is, and modern quality assurance makes sure that what you are building will return as much signal and as little noise as possible. At Meeshkan, we accompany teams that build modern software. So what are you building? [Let us know](./)!
